{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61967d9d",
   "metadata": {},
   "source": [
    "# Week 11 Part 2: Machine Learning\n",
    "\n",
    "You will use this notebook to answer questions from the machine learning portion of in the Week 11 quiz.\n",
    "\n",
    "# ABCD-ReproNim: Exploring machine learning concepts\n",
    "\n",
    "In this notebook, we will explore some concepts presented in [ABCD-ReproNim lecture 11b](https://youtu.be/LAddDaqUe0A):\n",
    "- ordering of imputation and scaling in cross-validation\n",
    "- effect of sample size on error distributions\n",
    "- ensembling\n",
    "- evaluation of estimators\n",
    "\n",
    "You will use this notebook to guide you in answering questions for this week's quiz. First let's import all of the libraries that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ea3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec  # For customizing plot layout\n",
    "import matplotlib.pyplot as plt  # For making plots to visualize our results\n",
    "import numpy as np  # For working with array data\n",
    "import pandas as pd  # For organizing results into dataframes\n",
    "import seaborn as sns  # For plotting\n",
    "import time  # For rudimentary executing timing\n",
    "\n",
    "from scipy.constants import golden_ratio  # For scaling the aspect ratio of our figures\n",
    "\n",
    "from sklearn.datasets import make_classification  # For generating a synthetic classification problem\n",
    "from sklearn.impute import KNNImputer  # For imputing missing values in our synthetic dataset\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate  # For splitting our data\n",
    "from sklearn.model_selection import LeaveOneGroupOut, RepeatedKFold, StratifiedKFold  # For splitting our data\n",
    "from sklearn.svm import SVC  # Support vector machine classifier\n",
    "from sklearn.metrics import accuracy_score  # To calculate...wait for it...the accuracy score\n",
    "from sklearn.ensemble import AdaBoostRegressor  # For exploring the concept of ensembling\n",
    "from sklearn.tree import DecisionTreeRegressor  # For exploring the concept of ensembling\n",
    "\n",
    "from nilearn import datasets  # For exploring the Haxby dataset mentioned in the lecture\n",
    "from nilearn.image import get_data  # See above comment\n",
    "from nilearn.image import index_img  # See above comment\n",
    "from nilearn.decoding import Decoder  # I could do this all day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba085bc",
   "metadata": {},
   "source": [
    "We'll set a random seed for re-executability. Let's choose a random state in honor of NDA Study 901. Can you figure out which study that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5678fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=901"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851893f",
   "metadata": {},
   "source": [
    "## Question #6: When should we do data imputation and scaling?\n",
    "\n",
    "In this problem, we will create a synthetic classification problem using scikit-learn's `make_classification` function and then train a support vector classifier. Real-world datasets often have missing data so we will simulate this by randomly inserting NaN values into our feature matrix. We will also split our dataset into train and test splits, evaluating our model on test data that it did not see during training. So we have several steps in our analysis pipeline (intentionally out of order):\n",
    "\n",
    "- evaluate model\n",
    "- split dataset into train and test splits\n",
    "- impute missing values\n",
    "- create dataset (including missing values)\n",
    "- train the model\n",
    "\n",
    "Some orderings of these steps aren't even possible (e.g. you can't split the data before you create it). But some are both possible and wrong. In the next two cells, we will repeat our analysis with two possible orderings of these steps and report the accuracy scores. Think about these two options and the lecture to answer question #6 in the quiz.\n",
    "\n",
    "#### Option A: impute then split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef028c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=100,\n",
    "    n_informative=20,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    flip_y=0.2,\n",
    ")\n",
    "\n",
    "# Choose random indexes to put NaN values into the data\n",
    "n_nans = int(X.size * 0.02)\n",
    "rng = np.random.default_rng(seed=RANDOM_STATE)\n",
    "index_nan = rng.choice(X.size, n_nans, replace=False)\n",
    "  \n",
    "# Add the NaN values to the feature matrix.\n",
    "X.ravel()[index_nan] = np.nan \n",
    "\n",
    "# Impute the missing values\n",
    "imputer = KNNImputer()\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy_1 = accuracy_score(y_test, y_pred)\n",
    "print(f\"With imputation before splitting, the accuracy {accuracy_1 * 100:4.1f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53177368",
   "metadata": {},
   "source": [
    "#### Option B: Split then impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45937256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=100,\n",
    "    n_informative=20,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    flip_y=0.1,\n",
    ")\n",
    "\n",
    "# Add the NaN values to the feature matrix.\n",
    "X.ravel()[index_nan] = np.nan \n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Impute the missing values\n",
    "imputer = KNNImputer()\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# fit the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy_2 = accuracy_score(y_test, y_pred)\n",
    "print(f\"With imputation before splitting, the accuracy score is {accuracy_2 * 100:4.1f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172587f2",
   "metadata": {},
   "source": [
    "\n",
    "## Question #7: Sample size affects test error distribution\n",
    "\n",
    "In this section, we are going to create more synthetic classification problems and evaluate `SVC` estimators on them. This time, we will vary the sample size and observe the effect on the test error distribution. Let's first write a function that will create the synthetic dataset and evaluate an `SVC` model on it using [stratified K-fold cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold). We will use the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) function to easily obtain test scores across all of the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svc_crossval_scores(n_samples=50, random_state=RANDOM_STATE):\n",
    "    # define dataset\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=100,\n",
    "        n_informative=20,\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        random_state=random_state,\n",
    "        flip_y=0.1,\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=20, random_state=RANDOM_STATE, shuffle=True)\n",
    "    scores = cross_val_score(SVC(), X, y, cv=cv)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da63042",
   "metadata": {},
   "source": [
    "Next, let get scores for a range of sample sizes. This will take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eca86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = [50, 100, 500, 1000, 5000]\n",
    "sample_scores = {\n",
    "    n_samp: get_svc_crossval_scores(n_samples=n_samp)\n",
    "    for n_samp in n_samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe98a23",
   "metadata": {},
   "source": [
    "Now we'll convert these scores to a pandas DataFrame (because seaborn likes pandas DataFrames) and center the scores by subtracting the mean of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(sample_scores)\n",
    "# df_scores = df_scores - df_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b2469",
   "metadata": {},
   "source": [
    "And we'll make a plot to look at the distribution of test accuracies. You'll use this plot to answer a question in the quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d08a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "_ = sns.swarmplot(data=df_scores, ax=ax, size=8)\n",
    "_ = ax.set_xlabel(\"n_samples\", fontsize=16)\n",
    "_ = ax.set_ylabel(r\"Centered Accuracy, $acc - \\langle acc \\rangle$\", fontsize=16)\n",
    "_ = ax.set_title(\"Distribution of cross-validated accuracy with increasing sample size\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63924765",
   "metadata": {},
   "source": [
    "## Questions #8 and #9: Evaluating different estimators on the Haxby dataset\n",
    "\n",
    "This section borrows heavily from [a nilearn example](https://nilearn.github.io/auto_examples/02_decoding/plot_haxby_different_estimators.html#sphx-glr-auto-examples-02-decoding-plot-haxby-different-estimators-py) that compares different classifiers in a visual object recognition decoding task. It has some minor modifications for plotting. Run these cells and then reflect on the output as you answer the corresponding quiz question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by loading data using nilearn dataset fetcher\n",
    "# by default 2nd subject data will be fetched\n",
    "haxby_dataset = datasets.fetch_haxby()\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First subject anatomical nifti image (3D) located is at: %s' %\n",
    "      haxby_dataset.anat[0])\n",
    "print('First subject functional nifti image (4D) is located at: %s' %\n",
    "      haxby_dataset.func[0])\n",
    "\n",
    "# load labels\n",
    "labels = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\n",
    "stimuli = labels['labels']\n",
    "\n",
    "# identify resting state (baseline) labels in order to be able to remove them\n",
    "resting_state = (stimuli == 'rest')\n",
    "\n",
    "# extract the indices of the images corresponding to some condition or task\n",
    "task_mask = np.logical_not(resting_state)\n",
    "\n",
    "# find names of remaining active labels\n",
    "categories = stimuli[task_mask].unique()\n",
    "\n",
    "# extract tags indicating to which acquisition run a tag belongs\n",
    "session_labels = labels['chunks'][task_mask]\n",
    "\n",
    "\n",
    "# Load the fMRI data\n",
    "# For decoding, standardizing is often very important\n",
    "mask_filename = haxby_dataset.mask_vt[0]\n",
    "func_filename = haxby_dataset.func[0]\n",
    "\n",
    "# Because the data is in one single large 4D image, we need to use\n",
    "# index_img to do the split easily.\n",
    "fmri_niimgs = index_img(func_filename, task_mask)\n",
    "classification_target = stimuli[task_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we define the various classifiers that we use\n",
    "classifiers = ['svc_l2', 'svc_l1', 'logistic_l1',\n",
    "               'logistic_l2', 'ridge_classifier']\n",
    "\n",
    "# Here we compute prediction scores and run time for all these\n",
    "# classifiers\n",
    "cv = LeaveOneGroupOut()\n",
    "classifiers_data = {}\n",
    "\n",
    "for classifier_name in sorted(classifiers):\n",
    "    classifiers_data[classifier_name] = {}\n",
    "    print(70 * '_')\n",
    "\n",
    "    # The decoder has as default score the `roc_auc`\n",
    "    decoder = Decoder(estimator=classifier_name, mask=mask_filename,\n",
    "                      standardize=True, cv=cv)\n",
    "    t0 = time.time()\n",
    "    decoder.fit(fmri_niimgs, classification_target, groups=session_labels)\n",
    "\n",
    "    classifiers_data[classifier_name] = {}\n",
    "    classifiers_data[classifier_name]['score'] = decoder.cv_scores_\n",
    "    classifiers_data[classifier_name]['map'] = decoder.coef_img_['house']\n",
    "\n",
    "    print(\"%10s: %.2fs\" % (classifier_name, time.time() - t0))\n",
    "    for category in categories:\n",
    "        print(\"    %14s vs all -- AUC: %1.2f +- %1.2f\" % (\n",
    "            category,\n",
    "            np.mean(classifiers_data[classifier_name]['score'][category]),\n",
    "            np.std(classifiers_data[classifier_name]['score'][category]))\n",
    "        )\n",
    "\n",
    "    # Adding the average performance per estimator\n",
    "    scores = classifiers_data[classifier_name]['score']\n",
    "    scores['AVERAGE'] = np.mean(list(scores.values()), axis=0)\n",
    "    classifiers_data[classifier_name]['score'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7809d1e",
   "metadata": {},
   "source": [
    "Aside: Note here we use the procedural interface to matplotlib\n",
    "as opposed to the object-oriented interfaces used above.\n",
    "Both of them work. The procedural interface is fast and very Matlab-like.\n",
    "The object-oriented interface can be more expressive if you want fine-grained control over what your plots look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "all_categories = np.sort(np.hstack([categories, 'AVERAGE']))\n",
    "tick_position = np.arange(len(all_categories))\n",
    "plt.yticks(tick_position + 0.25, all_categories)\n",
    "height = 0.12\n",
    "\n",
    "cmap = plt.get_cmap(\"tab10\").colors\n",
    "for i, (color, classifier_name) in enumerate(zip(cmap ,classifiers)):\n",
    "    score_means = [\n",
    "        np.mean(classifiers_data[classifier_name]['score'][category])\n",
    "        for category in all_categories\n",
    "    ]\n",
    "\n",
    "    plt.barh(tick_position, score_means,\n",
    "             label=classifier_name.replace('_', ' '),\n",
    "             height=height, color=color)\n",
    "    tick_position = tick_position + height\n",
    "\n",
    "plt.xlabel('Classification accuracy (AUC score)')\n",
    "plt.ylabel('Visual stimuli category')\n",
    "plt.xlim(xmin=0.5, xmax=1.0)\n",
    "plt.legend(loc='lower left', ncol=1, bbox_to_anchor=(0.9, 0.0))\n",
    "plt.title(\n",
    "    'Category-specific classification accuracy for different classifiers')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fad723",
   "metadata": {},
   "source": [
    "## Question #10: Introduction to ensembling\n",
    "\n",
    "Dr. Varaquaux discussed the FREM model in his lecture, which is an ensemble approach. In this question, we will explore an ensemble method called [AdaBoost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost). We will train three models on a sinusoidal dataset:\n",
    "\n",
    "- A simple decision tree (with `max_depth = 4`)\n",
    "- A complex decision tree (with `max_depth = 15`)\n",
    "- An ensemble of 300 simple decision trees (each with `max_depth = 4`)\n",
    "\n",
    "We will then plot each model's prediction as well as it's performance on train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.linspace(0, 6, 150)[:, np.newaxis]\n",
    "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.25, X.shape[0])\n",
    "\n",
    "# Fit regression models\n",
    "regr_1 = DecisionTreeRegressor(max_depth=4)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=15)\n",
    "regr_3 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=300, random_state=rng)\n",
    "\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "regr_3.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "y_3 = regr_3.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab0b84",
   "metadata": {},
   "source": [
    "Use [`RepeatedKFold`](https://scikit-learn.org/stable/modules/cross_validation.html#repeated-k-fold) to generate more model performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd34b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=10)\n",
    "cv_scores_1 = cross_validate(regr_1, X, y, cv=cv, return_train_score=True)\n",
    "cv_scores_2 = cross_validate(regr_2, X, y, cv=cv, return_train_score=True)\n",
    "cv_scores_3 = cross_validate(regr_3, X, y, cv=cv, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe7298",
   "metadata": {},
   "source": [
    "Store the train and test scores in separate dataframes. We will [`melt`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html) the DataFrames so that each model type is a different entry in the model column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_scores = pd.DataFrame({\n",
    "    \"Simple Tree\": cv_scores_1[\"train_score\"],\n",
    "    \"Complex Tree\": cv_scores_2[\"train_score\"],\n",
    "    \"Ensemble\": cv_scores_3[\"train_score\"],\n",
    "}).melt(var_name=\"model\")\n",
    "df_test_scores = pd.DataFrame({\n",
    "    \"Simple Tree\": cv_scores_1[\"test_score\"],\n",
    "    \"Complex Tree\": cv_scores_2[\"test_score\"],\n",
    "    \"Ensemble\": cv_scores_3[\"test_score\"],\n",
    "}).melt(var_name=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f60b16",
   "metadata": {},
   "source": [
    "Now plot the true function as well as the three estimates. Also plot the train and test set performance distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_height = 7\n",
    "fig_width = golden_ratio * fig_height\n",
    "fontsize=14\n",
    "fig = plt.figure(tight_layout=True, figsize=(fig_width, fig_height))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, :])\n",
    "ax1 = fig.add_subplot(gs[1, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "_ = sns.swarmplot(data=df_train_scores, x=\"model\", y=\"value\", size=3, ax=ax1)\n",
    "_ = sns.swarmplot(data=df_test_scores, x=\"model\", y=\"value\", size=4, ax=ax2)\n",
    "\n",
    "_ = ax1.set_title(\"Cross-validated training scores\", fontsize=fontsize)\n",
    "_ = ax2.set_title(\"Cross-validated test scores\", fontsize=fontsize)\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_ylabel(r\"$R^2$ score\", fontsize=fontsize)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.tick_params(axis=\"x\", which=\"major\", labelsize=fontsize)\n",
    "\n",
    "# Plot the results\n",
    "ax0.scatter(X, y, c=\"k\", alpha=0.8, label=\"training samples\")\n",
    "ax0.plot(X, y_1, c=\"g\", ls=\"-.\", alpha=0.8, label=\"Simple tree\", linewidth=2)\n",
    "ax0.plot(X, y_2, c=\"r\", ls=\"--\", alpha=0.8, label=\"Complex tree\", linewidth=2)\n",
    "ax0.plot(X, y_3, c=\"b\", ls=\"-.\", alpha=0.8, label=\"Simple tree ensemble\", linewidth=2)\n",
    "ax0.set_xlabel(\"data\", fontsize=fontsize)\n",
    "ax0.set_ylabel(\"target\", fontsize=fontsize)\n",
    "ax0.set_title(\"Boosted Decision Tree Regression\", fontsize=fontsize)\n",
    "_ = ax0.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083e7b3",
   "metadata": {},
   "source": [
    "You will use these plots to answer a question in this week's quiz. This example shows why ensemble learning is a popular strategy to address the bias-variance tradeoff (see [this example for a more detailed discussion of bias/variance in ensemble learning](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py)). Ensembling is also the inspiration for the [FREM pipeline](https://nilearn.github.io/decoding/frem.html) that Dr. Varoquaux mentioned in the lecture. See [this nilearn example](https://nilearn.github.io/auto_examples/02_decoding/plot_haxby_frem.html#sphx-glr-auto-examples-02-decoding-plot-haxby-frem-py) to use FREM on the Haxby dataset that you saw earlier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
